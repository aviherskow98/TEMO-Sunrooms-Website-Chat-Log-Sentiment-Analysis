{"cells":[{"cell_type":"code","execution_count":null,"id":"c0349548-3f54-45bf-9099-62e7277e8e86","metadata":{"id":"c0349548-3f54-45bf-9099-62e7277e8e86"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","from sqlalchemy import create_engine, text\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import seaborn as sns\n","\n","#import nltk\n","#nltk.download('vader_lexicon')\n","#nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","import nltk"]},{"cell_type":"markdown","id":"d14f2907-a1fe-4dc0-9690-abc7e376262f","metadata":{"id":"d14f2907-a1fe-4dc0-9690-abc7e376262f"},"source":["## Column definitions\n","\n","- **Chat Id**: unique id given to a chat (good for indexing)\n","  \n","- **Picked Up On**: datetime of when Visitor initially responds to welcome chat (default '12/31/1899 9:00:00 PM' if never occurs)\n","  \n","- **Created On**: datetime when Visitor opens chat\n","\n","- **Ended On**: datetime when Chat is ended (inactivity or closed by representative)\n","\n","- **Location**: location of Visitor (when available)\n","\n","- **Referrer**: The TEMO website page in which the user accesses the chat (hyperlink)\n","- **Lead Id**: If assigned lead type, unique lead id created for database purposes\n","- **Lead Type Id**: Type of lead\n","     - 0: not assigned a lead type\n","     - 1: Sales\n","     - 2: Service\n","     - 3: Other\n","- **Lead Type Name**: explained above\n","- **Original Referrer**: Website Page that lead them to the Referrer (helpful for ad campaign tracking)\n","- **Landing Referrer**: Same as Original Referrer except more complete data\n","- **Transcript Text**: transcript of the chat\n","- **Visitor Id**: Unique ID given to visitor (track if visitor returns)"]},{"cell_type":"code","execution_count":null,"id":"486dcb4b-9746-406e-9582-ab05d3e25d46","metadata":{"id":"486dcb4b-9746-406e-9582-ab05d3e25d46"},"outputs":[],"source":["#read data from csv\n","df_raw = pd.read_csv('/home/apm204/cs210/Final Project/TEMO_Sunrooms_Blazeo_Chats.csv')\n","#df_raw.describe()\n","#df_raw.dtypes\n","\n","df_path = r'/home/apm204/cs210/Final Project/raw_date_temo_sunrooms.xlsx'\n","df_raw.to_excel(df_path, index=False)\n","\n","#remove redundant, useless information that is repeated throughout entire dataset\n","df_raw.drop(['Company Id', 'Company Name', 'Company Key'], axis=1, inplace=True)\n","\n","## df_proc: dataframe used to show progression of processed data\n","df_proc = df_raw.copy()"]},{"cell_type":"markdown","id":"2b11aa0d-a986-40f0-8e19-f900feca064a","metadata":{"id":"2b11aa0d-a986-40f0-8e19-f900feca064a"},"source":["# I. Begin Processing Data\n"]},{"cell_type":"markdown","id":"7b85f322-6cee-46bd-8870-1c906328a205","metadata":{"jp-MarkdownHeadingCollapsed":true,"id":"7b85f322-6cee-46bd-8870-1c906328a205"},"source":["**Step 1**: Create numerical column 'visitor_engaged' based on the categorical 'Picked Up On' column."]},{"cell_type":"code","execution_count":null,"id":"cdeb57d1-2250-4e92-924b-193da9f8b46d","metadata":{"id":"cdeb57d1-2250-4e92-924b-193da9f8b46d"},"outputs":[],"source":["try:\n","\n","    #creating simpler columns to reflect activity of visitor in the chat\n","    df_proc['visitor_engaged'] = (df_proc['Picked Up On'] != '12/31/1899 9:00:00 PM').astype(int)\n","\n","    \"\"\"\n","    visitor_engaged:\n","        - 1 is visitor engaged in chat\n","        - 0 if visitor never engages in chat\n","    \"\"\"\n","\n","    #df_path = r'/home/apm204/cs210/Final Project/checker.csv'\n","    #df_proc[['Picked Up On', 'visitor_engaged']].to_csv(df_path, index=False)\n","    df_proc.drop(['Picked Up On'], axis=1, inplace=True)\n","\n","except KeyError:\n","    print(\"\\nPreventing KeyError from running cells out of order (all good)\\n\")"]},{"cell_type":"markdown","id":"f18116eb-5acd-4a6d-8b72-37181d961924","metadata":{"id":"f18116eb-5acd-4a6d-8b72-37181d961924"},"source":["**Step 2**: Drop 'Lead Type Name' as it is the same as 'Lead Type Id'"]},{"cell_type":"code","execution_count":null,"id":"fbca43f2-7915-4515-a4a0-1e63805ef3fd","metadata":{"id":"fbca43f2-7915-4515-a4a0-1e63805ef3fd"},"outputs":[],"source":["try:\n","    #Lead Type Name is just a label for Lead Type Id which provides chance for numeric analysis\n","    #Both provide same information, dropping Lead Type Name\n","    df_proc.drop(['Lead Type Name'], axis=1, inplace=True)\n","except KeyError:\n","    print(\"\\nPreventing KeyError from running cells out of order (all good)\\n\")\n"]},{"cell_type":"markdown","id":"a35d171b-6998-44ed-b678-9018ed4fca68","metadata":{"id":"a35d171b-6998-44ed-b678-9018ed4fca68"},"source":["**Step 3**: Advised by TEMO Sunrooms Representative that 'Original Referrer' possesses less important information when compared to 'Landing Referrer'. Dropping 'Original Referrer' as it will become negligible and difficult to run analysis since it is varied and categorical."]},{"cell_type":"code","execution_count":null,"id":"145c7f62-fc6c-442a-84b3-33d545755484","metadata":{"id":"145c7f62-fc6c-442a-84b3-33d545755484"},"outputs":[],"source":["try:\n","    #After discussing with TEMO Representative that provided access,\n","    #Original Referrer provides little value and is a subset of the data in Landing Referrer\n","    #Removing Original Referrer will maintain the information we have since it is contained in Landing Referrer\n","        #this also helps minimize the number of na records that need to be dealth with\n","    df_proc.drop(['Original Referrer'], axis=1, inplace=True)\n","except KeyError:\n","    print(\"\\nPreventing KeyError from running cells out of order (all good)\\n\")"]},{"cell_type":"markdown","id":"b9bc2cbd-b2fd-4605-870c-6f20bbe0120c","metadata":{"id":"b9bc2cbd-b2fd-4605-870c-6f20bbe0120c"},"source":["**Step 4**: Fill in na for 'Landing Referrer' and 'Referrer'"]},{"cell_type":"code","execution_count":null,"id":"75d5fcfb-57f8-45e5-922c-66a6abc9b09e","metadata":{"id":"75d5fcfb-57f8-45e5-922c-66a6abc9b09e","outputId":"2f528ef4-8b04-4bdd-a103-d67c86dc9098"},"outputs":[{"name":"stdout","output_type":"stream","text":["15.21% of rows in 'Landing Referrer' are na.\n","79.76% of visitors with no landing referrer were engaged in chat\n","\n","The largest referrer (https://www.google.com/) covers 37.98% of all non-na rows.\n","The second-largest referrer (https://www.temosunrooms.com/) covers 8.93% of all non-na rows.\n","length of df_visitor_id_na: 57\n","length of df_refer_na: 55\n","\n","The largest referrer (https://www.temosunrooms.com/) covers 17.78% of all non-na rows.\n","The second-largest referrer (https://www.temosunrooms.com/find-a-dealer/) covers 14.91% of all non-na rows.\n","The third-largest referrer (https://www.temosunrooms.com/contact/) covers 10.07% of all non-na rows.\n","\n","3.20% of rows have a 'Visitor Id' == 0 and NaN 'Referrer'\n"]}],"source":["#find the % of rows in 'Landing Referrer' that are na\n","print(f\"{len(df_proc[df_proc['Landing Referrer'].isna()])/len(df_proc)*100:.2f}% of rows in 'Landing Referrer' are na.\")\n","\n","#find the % of visitors that have no landing referrer that were inactive\n","avg_nr_inactive = df_proc[df_proc['Landing Referrer'].isna()]['visitor_engaged'].mean()\n","print(f'{avg_nr_inactive*100:.2f}% of visitors with no landing referrer were engaged in chat')\n","    # ~80%\n","    #this is a large portion of active visitors therefore it would be helpful to fill these na values rather than delete them\n","\n","\n","\n","#create dataframe, without rows that have na in 'Landing Referrer'\n","df_lr = df_proc.copy()\n","df_lr = df_lr[df_lr['Landing Referrer'].notna()]\n","\n","#group by different referrers to see if the mode is significant enough to be used to fill in the rows that are na\n","referrers = df_lr.groupby('Landing Referrer').size()\n","print(f'\\nThe largest referrer ({referrers.idxmax()}) covers {referrers.max()/len(df_lr)*100:.2f}% of all non-na rows.')\n","print(f'The second-largest referrer ({  referrers.nlargest(2).index[1]  }) covers {referrers.nlargest(2).iloc[1] / len(df_lr) * 100:.2f}% of all non-na rows.')\n","#google home page accounts for 40% of the referrers\n","    #second place (temo home page) accounts for less than 10%\n","#this sizable portion of the data means that we can fill na rows with google's home page\n","\n","#since ~15% of rows are na, this will make google home page account for 55% of all landing referrers\n","mode_referrer = referrers.idxmax()\n","df_proc.loc[df_proc['Landing Referrer'].isna(), 'Landing Referrer'] = mode_referrer\n","\n","\n","#the next step in cleaning/processing this dataset is dealing with the rows with na in 'Referrer'\n","#this tends to go hand-in-hand with the chats that have a 'Visitor Id' of 0\n","#we will approach these two columns together\n","\n","#begin by analyzing the similarities\n","\n","#dataset of chats with no visitor id (id = 0)\n","df_visitor_id_na = df_proc.copy()\n","df_visitor_id_na = df_visitor_id_na[df_visitor_id_na['Visitor Id']==0]\n","\n","print(f'length of df_visitor_id_na: {len(df_visitor_id_na)}')\n","df_visitor_id_na.head(5)\n","\n","#df_path = r'/home/apm204/cs210/Final Project/vis_id_0.csv'\n","#df_visitor_id_na.to_csv(df_path, index=False)\n","\n","\n","#dataset of chats with no referrer (referrer = na)\n","df_refer_na = df_proc.copy()\n","df_refer_na = df_refer_na[df_refer_na['Referrer'].isna()]\n","\n","print(f'length of df_refer_na: {len(df_refer_na)}')\n","df_refer_na.head(5)\n","\n","#df_path = r'/home/apm204/cs210/Final Project/refer_na.csv'\n","#df_refer_na.to_csv(df_path, index=False)\n","\n","\n","#There is plenty of overlap between these datasets\n","\n","#it can be seen that in both datasets there are subsequent rows with the same location;\n","#after inspection, these are the same chatters therefore they must be given the same visitor id\n","\n","#since each location has one cluster of chats, we can assign a visitor id to each location cluster\n","\n","#in order to fill the 'Referrer' column, we can first analyze the current makeup of the 'Referrer' column in the raw dataset\n","\n","#create dataframe, without rows that have na in 'Landing Referrer'\n","df_ref = df_proc.copy()\n","df_ref = df_ref[df_ref['Referrer'].notna()]\n","\n","#group by different referrers to see if the mode is significant enough to be used to fill in the rows that are na\n","referrers = df_ref.groupby('Referrer').size()\n","print(f'\\nThe largest referrer ({referrers.idxmax()}) covers {referrers.max()/len(df_ref)*100:.2f}% of all non-na rows.')\n","print(f'The second-largest referrer ({  referrers.nlargest(2).index[1]  }) covers {referrers.nlargest(2).iloc[1] / len(df_ref) * 100:.2f}% of all non-na rows.')\n","print(f'The third-largest referrer ({  referrers.nlargest(3).index[2]  }) covers {referrers.nlargest(3).iloc[2] / len(df_ref) * 100:.2f}% of all non-na rows.')\n","mode_referrer = referrers.idxmax()\n","\n","#check pct of rows we are filling in for\n","df_ref_na_vis_id_0 = df_proc[(df_proc['Referrer'].isna()) & (df_proc['Visitor Id']==0)]\n","pct_ref_na_vis_id_0 = len(df_ref_na_vis_id_0)/len(df_proc)*100\n","print(f\"\\n{pct_ref_na_vis_id_0:.2f}% of rows have a 'Visitor Id' == 0 and NaN 'Referrer'\")\n","\n","#since we are only affecting ~3% of the dataset, we can assign the mode 'Referrer' to the rows with NaN 'Referrer'\n","df_filtered = df_proc[df_proc['Referrer'].isna()]\n","vis_ids = set(df_proc[df_proc['Visitor Id'] != 0]['Visitor Id'])\n","locations = df_filtered['Location'].unique()\n","for location in locations:\n","    rand_int = np.random.randint(1400000000, 1600000000)\n","    while rand_int in vis_ids:\n","        rand_int = np.random.randint(1400000000, 1600000000)\n","    vis_ids.add(rand_int)\n","\n","    # Assign Visitor Id and Referrer for the current location in df_filtered\n","    df_filtered.loc[df_filtered['Location'] == location, 'Visitor Id'] = rand_int\n","    df_filtered.loc[df_filtered['Location'] == location, 'Referrer'] = mode_referrer\n","\n","# Update the original DataFrame with the changes\n","df_proc.update(df_filtered)"]},{"cell_type":"markdown","id":"499363c5-623a-45a5-b67d-c18bcfb6dea8","metadata":{"id":"499363c5-623a-45a5-b67d-c18bcfb6dea8"},"source":["**Step 5**: Generate Visitor ID's for chats without unique Visitor IDs"]},{"cell_type":"code","execution_count":null,"id":"9cae99d5-3317-4a06-90cf-687818b07045","metadata":{"id":"9cae99d5-3317-4a06-90cf-687818b07045"},"outputs":[],"source":["#we will look into the remaining rows that have 'Visitor Id' == 0\n","\n","df_proc[df_proc['Visitor Id']==0].head()\n","\n","#these 5 chats have no relation to each other, as shown by differing date/location/lead_type\n","#we can just assign a unique visitor id\n","\n","#update vis_ids\n","vis_ids = set(df_proc[df_proc['Visitor Id'] != 0]['Visitor Id'])\n","\n","#assign a unique visitor id to each of the 5 chats without a visitor id\n","for index in df_proc[df_proc['Visitor Id'] == 0].index:\n","    rand_int = np.random.randint(1400000000, 1600000000)\n","    while rand_int in vis_ids:\n","        rand_int = np.random.randint(1400000000, 1600000000)\n","\n","    df_proc.loc[index, 'Visitor Id'] = rand_int\n","    # Add the new visitor id to vis_ids to prevent duplication\n","    vis_ids.add(rand_int)"]},{"cell_type":"markdown","id":"29a0a267-eee0-489c-abab-7216703c2b0f","metadata":{"id":"29a0a267-eee0-489c-abab-7216703c2b0f"},"source":["**Step 6**: Verify that all processed data was done correctly using assert statements"]},{"cell_type":"code","execution_count":null,"id":"dccf22df-d79c-412a-a158-ffd33d1795c1","metadata":{"id":"dccf22df-d79c-412a-a158-ffd33d1795c1"},"outputs":[],"source":["#before analyzing the transcript text for sentiment, we will check if there are any rows without transcripts\n","df_proc[df_proc['Transcript Text'].isna()].head()\n","\n","#clearly we cannot analyze the sentiment on these chats as they have no transcript, we will discard them\n","df_proc = df_proc.dropna(subset=['Transcript Text'])\n","\n","assert not df_proc.isnull().values.any(), \"There are missing values in the data.\"\n","assert not (df_proc['Visitor Id'] == 0).all(), \"There are chats without a unique Visitor Id\"\n","assert not (df_proc['Referrer'].isna()).any(), \"There are missing Referrer values in the data.\"\n","assert ((df_proc['Lead Type Id'] >= 0) & (df_proc['Lead Type Id'] <= 3)).all(), \"'Lead Type Id' values are not all between 0 and 3\"\n","#there are no negative values in Sales\n","\n","#assert not na_df.isnull().values.any(), \"There are missing values in the data.\"\n","#there are no more missing values in the dataframe\n","\n","#assert (na_df['Sales'] >= 0).all(), \"'Sales' contains negative values.\"\n","#assert (na_df['Revenue'] >= 0).all(), \"'Revenue' contains negative values.\"\n","#there are no negative values in Sales or Revenue"]},{"cell_type":"markdown","id":"75651049-a74c-4400-8e91-0c1abeeb42bd","metadata":{"id":"75651049-a74c-4400-8e91-0c1abeeb42bd"},"source":["**Step 7**: Split 'Location' column into City, State, Country to break down into simpler data and possibly use for one-hot-encoding"]},{"cell_type":"code","execution_count":null,"id":"57eb5557-7fbb-4e7e-a951-eb7f83ae58ed","metadata":{"id":"57eb5557-7fbb-4e7e-a951-eb7f83ae58ed","outputId":"f374b82c-4ea6-4f85-ecb2-6858a2e6175c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Unknown City, Unknown Region, (US)' 'Unknown City, Unknown Region, (CN)'\n"," 'Unknown City, Unknown Region, (ES)' 'Unknown City, Unknown Region, (UA)'\n"," 'Unknown City, Unknown Region, (IN)' 'Unknown City, Unknown Region, (GB)'\n"," 'Unknown City, Unknown Region, (JP)']\n","[]\n","                  Location          City State Country\n","0  Unknown City, UNK, (US)  Unknown City   UNK      US\n","1     Little Elm, TX, (US)    Little Elm    TX      US\n","2     Little Elm, TX, (US)    Little Elm    TX      US\n","3  Unknown City, UNK, (US)  Unknown City   UNK      US\n","4  Unknown City, UNK, (US)  Unknown City   UNK      US\n"]}],"source":["#since the 'Location' column contains a lot of info that could be difficult to parse, we need to separate it\n","#is follows the format\n","    # 'some_city, some_state, (some_country)'\n","#this can be done with regex\n","\n","#regex pattern that reflects the one above\n","pattern = r'^([áA-Za-z\\s.]+),( )?([A-Za-z0-9]{1,3}),( )?\\(([A-Za-z]{2})\\)$'\n","\n","#rows where 'Location' does not match the pattern\n","invalid_locations = df_proc[~df_proc['Location'].str.match(pattern, na=False)]\n","\n","#rows with invalid 'Location' values\n","print(invalid_locations['Location'].unique())\n","\"\"\"\n","['Unknown City, Unknown Region, (US)' 'Unknown City, Unknown Region, (CN)'\n"," 'Unknown City, Unknown Region, (ES)' 'Unknown City, Unknown Region, (UA)'\n"," 'Unknown City, Unknown Region, (IN)' 'Unknown City, Unknown Region, (GB)'\n"," 'Unknown City, Unknown Region, (JP)']\n","\"\"\"\n","\n","#fixing the Unkown Region problem\n","#pattern of the locations above\n","pattern1 = r'^([A-Za-z\\s]+),\\s*Unknown Region,\\s*(\\([A-Z]{2}\\))$'\n","\n","# Use `str.replace` with regex capturing groups to substitute \"Unknown Region\" with \"UNK\"\n","df_proc['Location'] = df_proc['Location'].str.replace(pattern1, r'\\1, UNK, \\2', regex=True)\n","\n","\n","#check again\n","pattern = r'^([áA-Za-z\\s.]+),\\s?([A-Za-z0-9]{1,3}),\\s?\\(([A-Za-z]{2})\\)$'\n","invalid_locations = df_proc[~df_proc['Location'].str.match(pattern, na=False)]\n","print(invalid_locations['Location'].unique())\n","\"\"\"\n","[]\n","\"\"\"\n","\n","#works!\n","#now we want to split the location into 3 columns City, State, Country\n","#this works easily because the regex\n","    # \\1 is City\n","    # \\2 is State\n","    # \\3 is Country\n","# using 'str.extract' to create new columns based on the regex pattern\n","df_proc[['City', 'State', 'Country']] = df_proc['Location'].str.extract(pattern)\n","\n","#checking if it worked\n","print(df_proc[['Location', 'City', 'State', 'Country']].head())\n","\n","df_proc.drop(['Location'], axis=1, inplace=True)\n","\n","#all good!\n","#print(df_proc.dtypes)"]},{"cell_type":"markdown","id":"c8875017-3aca-4b24-aaeb-0432ec30fc9b","metadata":{"id":"c8875017-3aca-4b24-aaeb-0432ec30fc9b"},"source":["**Step 8**: Convert date columns into datetime format"]},{"cell_type":"code","execution_count":null,"id":"1tAya0GNnBAW","metadata":{"id":"1tAya0GNnBAW"},"outputs":[],"source":["# idea 1 implemented\n","# convert data colums to date time to allow easy calculation, providing format to avoid slow calculation\n","df_proc['Created On'] = pd.to_datetime(df_proc['Created On'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n","df_proc['Ended On'] = pd.to_datetime(df_proc['Ended On'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce')\n","\n","\n","# calculate the duration in minutes\n","df_proc['Chat Duration'] = (df_proc['Ended On'] - df_proc['Created On']).dt.total_seconds()/60\n","df_proc.loc[df_proc['Chat Duration'] < 0, 'Chat Duration'] = 0"]},{"cell_type":"markdown","id":"8b35d479-98f2-4b6f-9ea3-d37e75b895ce","metadata":{"id":"8b35d479-98f2-4b6f-9ea3-d37e75b895ce"},"source":["**Step 9**: Create a 1/0 numerical column detailing whether chat was closed due to inactivity or not"]},{"cell_type":"code","execution_count":null,"id":"594b6555-5d29-4039-b1b6-6d7a9a8a8f3f","metadata":{"id":"594b6555-5d29-4039-b1b6-6d7a9a8a8f3f"},"outputs":[],"source":["# idea 2 implemented\n","# Update 'Inactivity' column based on the presence of the inactivity phrase in the transcript\n","inactivity_phrase = \"This chatroom has been closed due to inactivity\"\n","df_proc['Inactivity'] = df_proc['Transcript Text'].apply(lambda x: 1 if inactivity_phrase in x else 0)"]},{"cell_type":"markdown","id":"580a990b-41a0-46d0-9775-94b8db52f0a1","metadata":{"id":"580a990b-41a0-46d0-9775-94b8db52f0a1"},"source":["**Step 10**: Create new dataframe with data about information given by visitor; 1/0 numerical true/false whether zip code, phone, email, or name was given using RegEx"]},{"cell_type":"code","execution_count":null,"id":"56007ffa-1f24-4fb1-8965-5e3791cb6b71","metadata":{"id":"56007ffa-1f24-4fb1-8965-5e3791cb6b71","outputId":"1c968544-0623-43c0-f115-8e3727890441"},"outputs":[{"name":"stdout","output_type":"stream","text":["   Has Email  Has Phone Number  Has Zip Code  Has Name\n","0          0                 0             0         1\n","1          1                 0             0         0\n","2          0                 0             0         0\n","3          1                 1             0         1\n","4          1                 1             1         1\n"]}],"source":["# idea 3 implemented\n","zip_code_pattern = r'\\b\\d{5}\\b(-\\d{4})?( \\d{4})?'  # Matches a 5-digit zip code\n","phone_pattern = r'\\b\\d{3}[-. ]?\\d{3}[-. ]?\\d{4}\\b'  # US phone format, e.g., 605-261-8272\n","email_pattern = r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,})'  # Standard email pattern\n","name_pattern = r'(((My name is |my name is |my names |my name\\'s |name: |name is |name\\'s |my name |fullname is |full name is )(([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)|([A-Z][a-z]+)|([A-Za-z]+\\s[A-Za-z]+)))|((Visitor: )(([A-Z][a-z]+\\s[A-Z][a-z]+)|([A-Za-z]+\\s[A-Za-z]+\\.))))' # name pattern following user text my name is\n","\n","# generate one-hot encoded columns for customer information (as discussed)\n","df_customer_info_one_hot = pd.DataFrame({\n","    'Has Email': df_proc['Transcript Text'].apply(lambda x: 1 if re.search(email_pattern, x) else 0),\n","    'Has Phone Number': df_proc['Transcript Text'].apply(lambda x: 1 if re.search(phone_pattern, x) else 0),\n","    'Has Zip Code': df_proc['Transcript Text'].apply(lambda x: 1 if re.search(zip_code_pattern, x) else 0),\n","    'Has Name': df_proc['Transcript Text'].apply(lambda x: 1 if re.search(name_pattern, x) else 0),\n","})\n","\n","\n","# Check the updated DataFrame\n","print(df_customer_info_one_hot.head())\n","\n","#ensure joining can occur by relating databases by IDs\n","df_customer_info_one_hot['Chat Id'] = df_proc['Chat Id']"]},{"cell_type":"markdown","id":"caa26d17-ebac-454c-bd12-960b8e89341d","metadata":{"id":"caa26d17-ebac-454c-bd12-960b8e89341d"},"source":["## Data Processed\n","\n","### **df_proc** is now processed with simplified columns without na/null/0 values\n","### **df_customer_info_one_hot** has data about the information given by the visitor"]},{"cell_type":"markdown","id":"b65577ba-f5e4-4d17-81e0-0f11605d2b64","metadata":{"id":"b65577ba-f5e4-4d17-81e0-0f11605d2b64"},"source":["# II. Sentiment Analysis"]},{"cell_type":"markdown","id":"9a9d154c-a5f2-4af3-a8f3-ae6b3dbcaa73","metadata":{"id":"9a9d154c-a5f2-4af3-a8f3-ae6b3dbcaa73"},"source":["a. Clean and process chatlog transcripts\n","\n","b. Tokenize and Stem chatlog transcripts\n","\n","c. Run VADER Sentiment Analysis\n","\n","d. Hold sentiment analysis in **df_sentiment**"]},{"cell_type":"markdown","source":["**Step 1**: Clean and process chatlog transcripts."],"metadata":{"id":"DByVPDI9Znq9"},"id":"DByVPDI9Znq9"},{"cell_type":"code","execution_count":null,"id":"rhSoSBbFnCVB","metadata":{"id":"rhSoSBbFnCVB","outputId":"4f57ea93-1c5d-4221-b986-16328acf8078"},"outputs":[{"name":"stdout","output_type":"stream","text":["     neg    neu    pos  compound\n","0  0.000  0.700  0.300    0.9565\n","1  0.000  0.823  0.177    0.9658\n","2  0.048  0.723  0.229    0.8053\n","3  0.000  0.823  0.177    0.9909\n","4  0.012  0.757  0.231    0.9894\n"]}],"source":["# initialize VADER for further processing\n","sia = SentimentIntensityAnalyzer()\n","# initialize stemmer\n","stemmer = PorterStemmer()\n","\n","# In this function I intend on cleaning the chat data.\n","# Since a chat might contain lots of extra data that us not useful and might\n","# even negatively influence the VADER analysis.\n","def clean_transcript(transcript):\n","    # remove timestamps (e.g., [1/2/2024 4:40:35 PM])\n","    transcript = re.sub(r\"\\[\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2}:\\d{2} [AP]M\\]\", \"\", transcript)\n","     # remove HTML garbage data\n","    transcript = re.sub(r'lt;.*?gt;', '', transcript)\n","    # remove extra whitespace\n","    transcript = re.sub(r\"\\s+\", \" \", transcript).strip()\n","    return transcript\n","\n"]},{"cell_type":"markdown","source":["**Step 2**: Tokenize and Stem Transcripts."],"metadata":{"id":"WPVZmCP2Z4sA"},"id":"WPVZmCP2Z4sA"},{"cell_type":"code","source":["def tokenize_transcript(transcript):\n","  # tokenize data into words\n","  tokens = word_tokenize(transcript)\n","  return tokens\n","\n","def stem_transcript(tokens):\n","  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n","  return \" \".join(stemmed_tokens)"],"metadata":{"id":"QH-j_WM0ZxsZ"},"id":"QH-j_WM0ZxsZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 3:** Run VADER Sentiment Analysis"],"metadata":{"id":"DWQ5U0BmaBtK"},"id":"DWQ5U0BmaBtK"},{"cell_type":"code","source":["# function to analyze sentiment\n","def analyze_transcript_sentiment(transcript):\n","    cleaned_text = clean_transcript(transcript)\n","    tokenized_text = tokenize_transcript(cleaned_text)\n","    stemmed_text = stem_transcript(tokenized_text)\n","    sentiment = sia.polarity_scores(stemmed_text)\n","    return sentiment"],"metadata":{"id":"PrHolg7zaBKQ"},"id":"PrHolg7zaBKQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Step 4**: Store sentiment analysis in **df_sentiment**"],"metadata":{"id":"trZz3SGwaLZb"},"id":"trZz3SGwaLZb"},{"cell_type":"code","source":["df_sentiment = df_proc['Transcript Text'].apply(analyze_transcript_sentiment).apply(pd.Series)\n","print(df_sentiment.head())\n","\n","#ensure joining can occur by relating databases by IDs\n","df_sentiment['Chat Id'] = df_proc['Chat Id']\n","\n","\n","#lowest_index = df_sentiment['compound'].idxmin()\n","#lowest_sentiment_transcript = df_proc.iloc[lowest_index]['Transcript Text']\n","#print(\"Full Transcript with the Lowest Sentiment:\")\n","#print(lowest_sentiment_transcript)\n","#print(sorted(df_sentiment['compound'])[1])"],"metadata":{"id":"3w21p8G2aK-G"},"id":"3w21p8G2aK-G","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7a16388c-0154-497e-9def-fdc15c85517e","metadata":{"id":"7a16388c-0154-497e-9def-fdc15c85517e"},"outputs":[],"source":["#df_path = r'/home/apm204/cs210/Final Project/df_proc.csv'\n","#df_proc.to_csv(df_path, index=False)\n","\n","#df_path = r'/home/apm204/cs210/Final Project/df_customer_info_one_hot.csv'\n","#df_customer_info_one_hot.to_csv(df_path, index=False)\n","\n","#df_path = r'/home/apm204/cs210/Final Project/df_sentiment.csv'\n","#df_sentiment.to_csv(df_path, index=False)\n","\n","#df_sentiment['Chat Id'] = df_proc['Chat Id']\n","#df_customer_info_one_hot['Chat Id'] = df_proc['Chat Id']"]},{"cell_type":"markdown","id":"VS41qEBLt0BT","metadata":{"id":"VS41qEBLt0BT"},"source":["## Current Data Frames:\n","### 1) **df_proc:** The main data frame with the extracted data\n","### 2) **df_customer_info_one_hot** The data frame containing a one hot encoded breakdown of what chats provided personal infomation such as zip code, name, address, phone number.\n","### 3) **df_sentiment** The data frame contating the sentiment information as received from VADER"]},{"cell_type":"markdown","id":"d63131ca-d385-4c7d-acfe-29c77151b84c","metadata":{"id":"d63131ca-d385-4c7d-acfe-29c77151b84c"},"source":["# III. Database"]},{"cell_type":"markdown","id":"53b463fa-fa4a-41d0-aa23-a887b09a501c","metadata":{"id":"53b463fa-fa4a-41d0-aa23-a887b09a501c"},"source":["**Step 1**: Create smaller data frames to be turned into SQL tables\n","\n","1. df_lead: Visitor referral history, mapping how they reached the TEMO Sunrooms Chat\n","2. df_location: Visitor location information grabbed from browser cookie data\n","3. df_chat: Chat information; length of chat, activity, whether the visitor engaged, and transcript\n","4. df_time: Time information; day of the week, month, weekend, chat duration, and whether the chat occurred in normal work hours"]},{"cell_type":"code","execution_count":null,"id":"6af672ef-3a5f-4f02-aefb-55cfbcbf3631","metadata":{"id":"6af672ef-3a5f-4f02-aefb-55cfbcbf3631"},"outputs":[],"source":["df_lead = df_proc[['Chat Id','Referrer', 'Lead Id', 'Lead Type Id', 'Landing Referrer', 'Visitor Id']]\n","df_location = df_proc[['Chat Id','City', 'State', 'Country']]\n","df_chat = df_proc[['Chat Id','Chat Duration', 'Inactivity', 'visitor_engaged', 'Transcript Text']]\n","\n","#df_path = r'/home/apm204/cs210/Final Project/df_lead.csv'\n","#df_lead.to_csv(df_path, index=False)\n","#df_path = r'/home/apm204/cs210/Final Project/df_location.csv'\n","#df_location.to_csv(df_path, index=False)\n","#df_path = r'/home/apm204/cs210/Final Project/df_chat.csv'\n","#df_chat.to_csv(df_path, index=False)\n","\n","#'created_on' in datetime format\n","df_proc['Created On'] = pd.to_datetime(df_proc['Created On'], errors='coerce')\n","df_time = pd.DataFrame()\n","\n","#extract main time features from 'created_on'\n","df_time['Chat Id'] = df_proc['Chat Id']\n","df_time['day_of_the_week'] = df_proc['Created On'].dt.day_name()  # Day of the week\n","df_time['month'] = df_proc['Created On'].dt.month_name()  # Month\n","\n","#one hot encoded weekend, 1 for weekend and 0 for weekday\n","df_time['weekend'] = (df_proc['Created On'].dt.weekday >= 5).astype(int)\n","#pull over chat duration\n","df_time['chat_duration'] = df_proc['Chat Duration']\n","\n","# Define work hours and days\n","def is_working_hours(datetime):\n","    # Check if day is Monday to Friday (0-4) and time is between 08:00 and 18:00\n","    return 0 <= datetime.weekday() <= 4 and datetime.hour >= 8 and datetime.hour < 18\n","\n","df_time['work_hours'] = df_proc['Created On'].apply(lambda x: 1 if is_working_hours(x) else 0)\n","\n","#print the dataframe to check\n","#print(df_time.head())"]},{"cell_type":"markdown","id":"b673632d-9634-4b72-a7a3-b347cf413321","metadata":{"id":"b673632d-9634-4b72-a7a3-b347cf413321"},"source":["**Step 2**: Process all column names to be lowercase with '_' instead of spaces; write data frames to csv."]},{"cell_type":"code","execution_count":null,"id":"1d9f1071-7629-4027-9daa-e54cb9ebb331","metadata":{"id":"1d9f1071-7629-4027-9daa-e54cb9ebb331"},"outputs":[],"source":["def process_columns(df_var):\n","    cols = df_var.columns\n","    lower_cols = [col.lower() for col in cols]\n","    processed_cols = [col.replace(\" \", \"_\") for col in lower_cols]\n","    return processed_cols"]},{"cell_type":"code","execution_count":null,"id":"9ab0242d-f8cf-4ac1-a283-66f530276ca0","metadata":{"id":"9ab0242d-f8cf-4ac1-a283-66f530276ca0","outputId":"409881ae-6c56-44d1-97d2-b9c48de38c96"},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['chat_id', 'chat_duration', 'inactivity', 'visitor_engaged',\n","       'transcript_text'],\n","      dtype='object')\n","Index(['has_email', 'has_phone_number', 'has_zip_code', 'has_name', 'chat_id'], dtype='object')\n","Index(['chat_id', 'referrer', 'lead_id', 'lead_type_id', 'landing_referrer',\n","       'visitor_id'],\n","      dtype='object')\n","Index(['chat_id', 'city', 'state', 'country'], dtype='object')\n","Index(['neg', 'neu', 'pos', 'compound', 'chat_id'], dtype='object')\n","Index(['chat_id', 'day_of_the_week', 'month', 'weekend', 'chat_duration',\n","       'work_hours'],\n","      dtype='object')\n"]}],"source":["df_chat.columns = process_columns(df_chat)\n","df_customer_info_one_hot.columns = process_columns(df_customer_info_one_hot)\n","df_lead.columns = process_columns(df_lead)\n","df_location.columns = process_columns(df_location)\n","df_sentiment.columns = process_columns(df_sentiment)\n","df_time.columns = process_columns(df_time)\n","\n","print(df_chat.columns)\n","print(df_customer_info_one_hot.columns)\n","print(df_lead.columns)\n","print(df_location.columns)\n","print(df_sentiment.columns)\n","print(df_time.columns)\n","\n","df_path = r'/home/apm204/cs210/Final Project/df_chat.csv'\n","df_chat.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_customer_info_one_hot.csv'\n","df_customer_info_one_hot.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_lead.csv'\n","df_lead.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_location.csv'\n","df_location.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_sentiment.csv'\n","df_sentiment.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_time.csv'\n","df_time.to_csv(df_path, index=False)"]},{"cell_type":"markdown","id":"2a2b43dc-a51b-4107-9fa1-790c4a52991e","metadata":{"id":"2a2b43dc-a51b-4107-9fa1-790c4a52991e"},"source":["# SQL TABLE LIST\n","\n","### 1) **df_lead:** Referrer information (access link and where they discovered chatbot), Visitor Id (check for repeats), Lead type (reason for chat)\n","### 2) **df_customer_info_one_hot:** The data frame containing a one hot encoded breakdown of what chats provided personal infomation such as zip code, name, email address, phone number.\n","### 3) **df_sentiment:** The data frame contating the sentiment information as received from VADER\n","### 4) **df_location:** location information including city, state, country (UNK for unknown)\n","### 5) **df_chat:** chat logistical information including time length of chat, use active/inactive, and the log of the chat\n","### 6) **df_time:** with day of the week, month, time of day to decide whether certain times of day have peaks in use retention and sentiment"]},{"cell_type":"markdown","id":"6f3c9fe5-312d-402a-b767-0aae64b7b29d","metadata":{"id":"6f3c9fe5-312d-402a-b767-0aae64b7b29d"},"source":["**Step 3**: Create SQL Engine with 6 (unique) data frames, connected by 'chat_id' column"]},{"cell_type":"code","execution_count":null,"id":"cf83c83e-9c08-4808-802c-28f338649ffc","metadata":{"id":"cf83c83e-9c08-4808-802c-28f338649ffc","outputId":"66bd37f3-c7ac-492a-f29e-ff3f54ee0b48"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('test_table',), ('df_proc',), ('df_clusters2',), ('df_clusters',), ('df_lead',), ('df_customer_info_one_hot',), ('df_sentiment',), ('df_location',), ('df_chat',), ('df_time',)]\n"]}],"source":["#ls /home/apm204/cs210/Final\\ Project/\n","\n","#from sqlalchemy import create_engine, text\n","#create SQLite engine\n","#engine = create_engine('sqlite:////home/apm204/cs210/Final Project/chat_logs.db')\n","#with engine.connect() as conn:\n","    #conn.execute(text(\"CREATE TABLE IF NOT EXISTS test_table (id INTEGER PRIMARY KEY, name TEXT);\"))\n","#with engine.connect() as conn:\n","    #result = conn.execute(text(\"SELECT name FROM sqlite_master WHERE type='table';\"))\n","    #print(result.fetchall())  # This should include 'test_table' if it was created successfully\n","\n","#create a SQL Alchemy engine\n","engine = create_engine('sqlite:////home/apm204/cs210/Final Project/chat_logs.db')\n","\n","#store each dataframe as a SQL table\n","df_lead.to_sql('df_lead', engine, index=False, if_exists='replace')\n","df_customer_info_one_hot.to_sql('df_customer_info_one_hot', engine, index=False, if_exists='replace')\n","df_sentiment.to_sql('df_sentiment', engine, index=False, if_exists='replace')\n","df_location.to_sql('df_location', engine, index=False, if_exists='replace')\n","df_chat.to_sql('df_chat', engine, index=False, if_exists='replace')\n","df_time.to_sql('df_time', engine, index=False, if_exists='replace')\n","\n","#verify tables creation\n","with engine.connect() as conn:\n","    result = conn.execute(text(\"SELECT name FROM sqlite_master WHERE type='table';\"))\n","    print(result.fetchall())"]},{"cell_type":"markdown","id":"a6e25fe8-ce1c-467d-9b60-fdc167d8813d","metadata":{"id":"a6e25fe8-ce1c-467d-9b60-fdc167d8813d"},"source":["# IV. SQL Database Analysis"]},{"cell_type":"markdown","id":"0bd77e78-2c05-47e2-8a09-5720d0235f26","metadata":{"id":"0bd77e78-2c05-47e2-8a09-5720d0235f26"},"source":["Use SQL Commands to find possible trends in data with joins and groups between different tables\n","\n","(found trends for sentiment with respect to activity, lead type, and day of the week)"]},{"cell_type":"code","execution_count":null,"id":"6e7f837b-6ba1-4d4e-8569-5c5734c03866","metadata":{"id":"6e7f837b-6ba1-4d4e-8569-5c5734c03866","outputId":"b153ac30-3d69-4aac-c038-854740e8bab6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","This query checks if chats created during work hours (8am-6pm) have different sentiment\n","   work_hours  avg_sentiment\n","0           0       0.929859\n","1           1       0.942392\n","\n","\n","Sentiment Based on User Information (df_customer_info_one_hot)\n","    has_email  has_phone_number  has_zip_code  has_name  avg_sentiment\n","0           0                 0             0         0       0.867769\n","1           0                 0             0         1       0.947689\n","2           0                 0             1         0       0.940002\n","3           0                 0             1         1       0.975829\n","4           0                 1             0         0       0.936541\n","5           0                 1             0         1       0.950554\n","6           0                 1             1         0       0.971176\n","7           0                 1             1         1       0.981796\n","8           1                 0             0         0       0.953666\n","9           1                 0             0         1       0.971971\n","10          1                 0             1         0       0.973915\n","11          1                 0             1         1       0.981656\n","12          1                 1             0         0       0.960474\n","13          1                 1             0         1       0.971336\n","14          1                 1             1         0       0.978753\n","15          1                 1             1         1       0.982840\n","\n","\n","Sentiment Based on Referrer\n","                                              referrer  avg_sentiment\n","0    https://apex.live/Pages/Chat.aspx?initiatedBy=...       0.795900\n","1                              https://duckduckgo.com/       0.846600\n","2                              https://www.google.com/       0.753710\n","3                        https://www.temosunrooms.com/       0.912822\n","4     https://www.temosunrooms.com/#gallery-pergolas-3       0.964400\n","..                                                 ...            ...\n","231       https://www.temosunrooms.com/value_view.htm/       0.952100\n","232  https://www.temosunrooms.com/vertical-wall-sys...       0.916300\n","233             https://www.temosunrooms.com/warranty/       0.943311\n","234  https://www.temosunrooms.com/what-is-differenc...       0.850100\n","235   https://www.temosunrooms.com/window-wall-system/       0.905171\n","\n","[236 rows x 2 columns]\n","referrer\n","https://www.temosunrooms.com/                       334\n","https://www.temosunrooms.com/find-a-dealer/         234\n","https://www.temosunrooms.com/contact/               158\n","https://www.temosunrooms.com/warranty/               73\n","https://www.temosunrooms.com/sunrooms/               64\n","https://www.temosunrooms.com/projects/               48\n","https://www.temosunrooms.com/help/                   39\n","https://www.temosunrooms.com/pergolas/               32\n","https://www.temosunrooms.com/current-promotions/     22\n","https://www.temosunrooms.com/patio-covers/           20\n","dtype: int64\n","\n","\n","Sentiment based on Lead Type\n","   lead_type_id  avg_sentiment\n","0             0       0.906917\n","1             1       0.984410\n","2             2       0.967550\n","3             3       0.943512\n"]}],"source":["#print('This query will help you determine if sentiment varies by day.')\n","#This query will help you determine if sentiment varies by day.\n","\n","query = \"\"\"\n","SELECT t.day_of_the_week, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_time t ON s.chat_id = t.chat_id\n","GROUP BY t.day_of_the_week;\n","\"\"\"\n","sentiment_day_of_week = pd.read_sql(query, engine)\n","#print(sentiment_day_of_week)\n","\n","\n","print('\\n\\nThis query checks if chats created during work hours (8am-6pm) have different sentiment')\n","#This query checks if chats created during work hours (8am-6pm) have different sentiment\n","\n","query = \"\"\"\n","SELECT t.work_hours, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_time t ON s.chat_id = t.chat_id\n","GROUP BY t.work_hours;\n","\"\"\"\n","sentiment_work_hours = pd.read_sql(query, engine)\n","print(sentiment_work_hours)\n","\n","\n","#print('\\n\\nThis query checks if longer chats have higher sentiment')\n","#This query checks if longer chats have higher sentiment\n","\n","query = \"\"\"\n","SELECT c.chat_duration, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_chat c ON s.chat_id = c.chat_id\n","GROUP BY c.chat_duration\n","ORDER BY c.chat_duration;\n","\"\"\"\n","sentiment_chat_duration = pd.read_sql(query, engine)\n","#print(sentiment_chat_duration)\n","\n","\n","print('\\n\\nSentiment Based on User Information (df_customer_info_one_hot)')\n","#Sentiment Based on User Information (df_customer_info_one_hot)\n","\n","query = \"\"\"\n","SELECT ci.has_email, ci.has_phone_number, ci.has_zip_code, ci.has_name, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_customer_info_one_hot ci ON s.chat_id = ci.chat_id\n","GROUP BY ci.has_email, ci.has_phone_number, ci.has_zip_code, ci.has_name;\n","\"\"\"\n","sentiment_customer_info = pd.read_sql(query, engine)\n","print(sentiment_customer_info)\n","\n","\n","#print('\\n\\nSentiment Based on Location (Country)')\n","#Sentiment Based on Location (Country)\n","\n","query = \"\"\"\n","SELECT l.country, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_location l ON s.chat_id = l.chat_id\n","GROUP BY l.country;\n","\"\"\"\n","sentiment_country = pd.read_sql(query, engine)\n","#print(sentiment_country)\n","\n","### COULD BE GOOD\n","# english speaking vs non-english speaking\n","\n","\n","#print('\\n\\nSentiment Based on Location (State)')\n","#Sentiment Based on Location (State)\n","\n","query = \"\"\"\n","SELECT l.state, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_location l ON s.chat_id = l.chat_id\n","WHERE l.country = 'US'\n","GROUP BY l.state;\n","\"\"\"\n","sentiment_state = pd.read_sql(query, engine)\n","#print(sentiment_state)\n","\n","\n","print('\\n\\nSentiment Based on Referrer')\n","#Sentiment Based on Referrer\n","\n","query = \"\"\"\n","SELECT l.referrer, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_lead l ON s.chat_id = l.chat_id\n","GROUP BY l.referrer;\n","\"\"\"\n","sentiment_referrer = pd.read_sql(query, engine)\n","print(sentiment_referrer)\n","\n","sentiment_referrer[sentiment_referrer['referrer']=='https://www.temosunrooms.com/']\n","\n","len(df_lead[df_lead['referrer']=='https://www.temosunrooms.com/window-wall-system/'])\n","\n","top_referrers = df_lead.groupby('referrer').size().sort_values(ascending=False).head(10)\n","\n","print(top_referrers)\n","\n","##could be a banger\n","# temosunrooms ads are working and bringing in the right people\n","# external links are negative leads\n","\n","\n","#print('\\n\\nSentiment for Returning Visitors (Duplicate Visitor IDs):')\n","#Sentiment for Returning Visitors (Duplicate Visitor IDs):\n","query = \"\"\"\n","SELECT v.visitor_id, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_lead v ON s.chat_id = v.chat_id\n","GROUP BY v.visitor_id\n","HAVING COUNT(v.visitor_id) > 1;\n","\"\"\"\n","sentiment_returning_visitors = pd.read_sql(query, engine)\n","#print(sentiment_returning_visitors)\n","\n","#sentiment_returning_visitors['avg_sentiment'].mean()\n","\n","\n","#print('\\n\\nSentiment based on Activity')\n","#Sentiment based on Activity\n","\n","query = \"\"\"\n","SELECT c.inactivity, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_chat c ON s.chat_id = c.chat_id\n","GROUP BY c.inactivity;\n","\"\"\"\n","sentiment_inactivity = pd.read_sql(query, engine)\n","#print(sentiment_inactivity)\n","\n","#not bad, look for retention\n","\n","\n","print('\\n\\nSentiment based on Lead Type')\n","#Sentiment based on Lead Type\n","\n","query = \"\"\"\n","SELECT l.lead_type_id, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_lead l ON s.chat_id = l.chat_id\n","GROUP BY l.lead_type_id;\n","\"\"\"\n","sentiment_lead_type_id = pd.read_sql(query, engine)\n","print(sentiment_lead_type_id)\n","\n","## shows a need to diagnose a customer inquiry to best help\n","## names are helpful but without properly diagnosing an issue first, sentiment significantly lacks\n","\n","## sales (1) being the highest sentiment shows maybe where interests lie, money making over customer relations\n","## outsourced chatbot configuration may cause this as higher sales lead to greater reporting\n","## seems to have happened at the detriment of visitors that are not seeking sales\n","\n","\n","#print('\\n\\nSentiment for Returning Visitors (Duplicate Visitor IDs):')\n","#Sentiment for Returning Visitors (Duplicate Visitor IDs):\n","\n","query = \"\"\"\n","SELECT v.visitor_id, AVG(s.compound) as avg_sentiment\n","FROM df_sentiment s\n","JOIN df_lead v ON s.chat_id = v.chat_id\n","GROUP BY v.visitor_id\n","HAVING COUNT(v.visitor_id) > 1;\n","\"\"\"\n","sentiment_returning_visitors = pd.read_sql(query, engine)\n","#print(sentiment_returning_visitors)\n","#meh"]},{"cell_type":"markdown","id":"08fe9c43-3a1a-40c9-96d9-15839a1a42be","metadata":{"id":"08fe9c43-3a1a-40c9-96d9-15839a1a42be"},"source":["### What was found?\n","\n","- Trend between sentiment compound score and referrer, non-TEMO links have lower sentiment\n","\n","- Lead Type affects sentiment compound score as chats without a defined Lead Type have lower sentiment than those that do. Additionally, revenue generating avenues (Sales and Service leads) have a higher satisfaction [this can be due to training, visitor interest, targeting campaigns, etc.\n","\n","- Weekdays during normal working hours leads to chats with higher sentiment\n","\n","- The more information a user provides, the higher the sentiment [beneficial for user due to a better chat and TEMO Sunrooms due to user information]"]},{"cell_type":"code","execution_count":null,"id":"74829e4c-6879-4551-8cb0-f130edecfd56","metadata":{"scrolled":true,"id":"74829e4c-6879-4551-8cb0-f130edecfd56","outputId":"e7a6fec6-2696-492e-c12d-b52b610fb165"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_1773258/3644104793.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_lead.loc[:, 'temo_landing_referrer'] = df_lead['landing_referrer'].str.contains('temo', case=False, na=False).astype(int)\n"]}],"source":["df_lead.loc[:, 'temo_landing_referrer'] = df_lead['landing_referrer'].str.contains('temo', case=False, na=False).astype(int)\n","\n","#df_lead['google_landing_referrer'] = df_lead['landing_referrer'].str.contains('google', case=False, na=False).astype(int)\n","#df_lead['other_landing_referrer'] = (~df_lead['temo_landing_referrer'].astype(bool) & ~df_lead['google_landing_referrer'].astype(bool)).astype(int)\n","#df_lead['referrer_type'] =\n","\n","df_customer_info_one_hot.loc[:, 'has_city'] = (df_location['city'] != 'Unknown City').astype(int)\n","df_customer_info_one_hot.loc[:, 'has_state'] = (df_location['state'] != 'UNK').astype(int)\n","\n","#mapping dictionary for the days of the week\n","day_mapping = {\n","    'Monday': 1,\n","    'Tuesday': 2,\n","    'Wednesday': 3,\n","    'Thursday': 4,\n","    'Friday': 5,\n","    'Saturday': 6,\n","    'Sunday': 7\n","}\n","#apply the mapping to the 'day_of_the_week' column\n","df_time['day_of_the_week_num'] = df_time['day_of_the_week'].map(day_mapping)\n","\n","month_mapping = {\n","    'January': 1,\n","    'February': 2,\n","    'March': 3,\n","    'April': 4,\n","    'May': 5,\n","    'June': 6,\n","    'July': 7\n","}\n","#apply the mapping to the 'month' column\n","df_time['month_num'] = df_time['month'].map(month_mapping)"]},{"cell_type":"markdown","id":"05e6156e-30db-4ff8-82f0-34196077b432","metadata":{"id":"05e6156e-30db-4ff8-82f0-34196077b432"},"source":["# V. Perform K-Means Clustering"]},{"cell_type":"code","execution_count":null,"id":"ed9e93c6-54d1-4ab3-92ce-a40e133b19dc","metadata":{"id":"ed9e93c6-54d1-4ab3-92ce-a40e133b19dc"},"outputs":[],"source":["#select the relevant features for clustering\n","X = pd.concat([\n","    df_chat[['chat_duration', 'inactivity']],\n","    df_customer_info_one_hot[['has_email', 'has_phone_number', 'has_zip_code', 'has_name', 'has_city', 'has_state']],\n","    df_lead[['lead_type_id', 'temo_landing_referrer']],\n","    df_time[['day_of_the_week_num', 'month_num', 'weekend', 'work_hours']]\n","], axis=1)\n","\n","df_proc['compound'] = df_sentiment['compound']\n","X['compound'] = df_sentiment['compound']  # Add sentiment score\n","\n","#scale the features (important for K-Means)\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","#apply K-Means clustering (let's assume k=3 for this example)\n","kmeans = KMeans(n_clusters=6, random_state=42)\n","kmeans.fit(X_scaled)\n","\n","#add the cluster labels to the dataframe\n","df_proc['cluster'] = kmeans.labels_\n","\n","#analyze the clusters (e.g., find out which features drive sentiment in each cluster)\n","#look at the cluster centers to see the average values of the features for each cluster\n","cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n","print(cluster_centers)\n","\n","#visualize the clusters (2D using PCA for simplicity)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","cluster_palette = {\n","    0: '#FF7F7F',  # Soft Red\n","    1: '#7FB3FF',  # Soft Blue\n","    2: '#7FFF7F',  # Soft Green\n","    3: '#FFBF7F',  # Soft Orange\n","    4: '#BF7FFF',  # Soft Purple\n","    5: '#FF7FBF'   # Soft Pink\n","}\n","\n","\n","\n","#plot the clusters\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df_proc['cluster'], palette=cluster_palette, s=100)\n","plt.title('Customer Segmentation Using K-Means Clustering')\n","plt.show()\n","\n","#plt.figure(figsize=(10, 6))\n","#sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df_proc['cluster'], palette=cluster_palette, s=100)\n","#plt.title('Customer Segmentation Using K-Means Clustering')\n","#visualization_path = r'cluster_visualization.png'\n","#plt.savefig(visualization_path)\n","\n","#analyze clusters based on sentiment\n","cluster_sentiment = df_proc.groupby('cluster')['compound'].mean()\n","print(cluster_sentiment)\n","\n","#df_customer_info_one_hot.columns\n","\n","#df_path = r'/home/apm204/cs210/Final Project/df_proc.csv'\n","#df_proc.to_csv(df_path, index=False)"]},{"cell_type":"markdown","id":"ad4bfe2c-d34c-4914-b268-ee4eba00d68a","metadata":{"id":"ad4bfe2c-d34c-4914-b268-ee4eba00d68a"},"source":["**# of Clusters: 5**\n","\n","Used 5 clusters as it ended up yielding the most information in developing a TEMO Sunrooms ideal customer along with providing information about the most important avenues of capturing leads.\n","\n","Will now analyze information about these clusters"]},{"cell_type":"markdown","id":"8eeda404-362b-408b-b8c5-a0cfeca3606a","metadata":{"id":"8eeda404-362b-408b-b8c5-a0cfeca3606a"},"source":["**Substep**: Create a combined_df in order to succinctly visualize trends in data"]},{"cell_type":"code","execution_count":null,"id":"c9cf8407-6ed4-42c1-aa81-fea510f7b32d","metadata":{"id":"c9cf8407-6ed4-42c1-aa81-fea510f7b32d","outputId":"4ff5f5ed-a729-4ec8-e537-e07d396cfffd"},"outputs":[{"data":{"text/plain":["1620"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["# List of DataFrames\n","dfs = [df_time, df_chat, df_customer_info_one_hot, df_lead, df_location, df_sentiment, df_proc]\n","\n","# Concatenate DataFrames along columns (axis=1)\n","combined_df = pd.concat(dfs, axis=1)\n","\n","#df_path = r'/home/apm204/cs210/Final Project/combined_df.csv'\n","#combined_df.to_csv(df_path, index=False)\n","\n","#combined_df.columns\n","combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n","\n","combined_df['total_info'] = combined_df['has_email'] + combined_df['has_phone_number'] + combined_df['has_zip_code'] + combined_df['has_name']\n","\n","#cluster_groups = combined_df.groupby('cluster')\n","#print(cluster_groups.get_group(2))\n","\n","df_clusters = pd.DataFrame()\n","df_clusters['chat_id'] = df_lead['chat_id']\n","df_clusters['clusters'] = kmeans.labels_\n","\n","df_clusters.to_sql('df_clusters', engine, index=False, if_exists='replace')"]},{"cell_type":"code","execution_count":null,"id":"8db6ca31-bf5d-4f65-a111-b4847961aee1","metadata":{"id":"8db6ca31-bf5d-4f65-a111-b4847961aee1"},"outputs":[],"source":["#plot histograms for each feature (chat_duration, visitor_engaged, etc.) per cluster\n","features = ['chat_duration', 'inactivity', 'has_email', 'has_phone_number', 'has_zip_code', 'has_name', 'lead_type_id', 'has_city', 'has_state', 'temo_landing_referrer', 'day_of_the_week_num', 'month_num', 'weekend', 'work_hours', 'total_info']\n","combined_df['unk_city'] = (combined_df['city'] == 'Unknown City').astype(int)\n","\n","###look at combined df\n","###make new referrer column 0-temo;1-google;2-other\n","### check what the spread is\n","for feature in features:\n","    plt.figure(figsize=(10, 6))\n","    sns.boxplot(x=combined_df['cluster'], y=combined_df[feature], palette='viridis')\n","    plt.title(f'Distribution of {feature} by Cluster')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"82c4caff-9817-432c-a9d9-d57672590226","metadata":{"id":"82c4caff-9817-432c-a9d9-d57672590226"},"outputs":[],"source":["#### BAD USELESS\n","\n","\n","subset_combined_df = combined_df.copy()\n","subset_combined_df = subset_combined_df[subset_combined_df['chat_duration'] <= 30]\n","subset_combined_df = subset_combined_df[subset_combined_df['compound'] >= 0.85]\n","\n","\n","# Create a scatterplot\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(data=subset_combined_df, x='chat_duration', y='compound', hue='cluster', palette='Set1', s=100, edgecolor='black')\n","\n","# Add labels and title\n","plt.title('Chat Duration vs Compound Score by Clusters', fontsize=14)\n","plt.xlabel('Chat Duration', fontsize=12)\n","plt.ylabel('Compound Score', fontsize=12)\n","\n","# Show the plot\n","plt.legend(title='Clusters')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"d34fec12-4514-4e12-84a9-1f34009e2d65","metadata":{"id":"d34fec12-4514-4e12-84a9-1f34009e2d65"},"outputs":[],"source":["#df_path = r'/home/apm204/cs210/Final Project/df_lead.csv'\n","#df_lead.to_csv(df_path, index=False)\n","\n","combined_df['temo_landing_referrer'] = df_lead['temo_landing_referrer']\n","combined_df['google_landing_referrer'] = df_lead['google_landing_referrer']\n","combined_df['other_landing_referrer'] = df_lead['other_landing_referrer']\n","\n","\n","print(combined_df.groupby('cluster')['temo_landing_referrer'].mean())\n","print(combined_df.groupby('cluster')['google_landing_referrer'].mean())\n","print(combined_df.groupby('cluster')['other_landing_referrer'].mean())\n","\n","# Plot histograms for each feature (chat_duration, visitor_engaged, etc.) per cluster\n","#features = ['temo_landing_referrer', 'google_landing_referrer', 'other_referrer']\n","#for feature in features:\n","    #plt.figure(figsize=(10, 6))\n","    #sns.scatterplot(x=combined_df['cluster'], y=df_lead[feature], palette='viridis')\n","    #plt.title(f'Distribution of {feature} by Cluster')\n","    #plt.show()\n"]},{"cell_type":"markdown","id":"f84571dd-9e3c-4bdb-81c3-1f66335ddb1a","metadata":{"id":"f84571dd-9e3c-4bdb-81c3-1f66335ddb1a"},"source":["i like the 5 clusters (1&2 being the best) as they show that Google users are less likely to give info and lead to slightly worse chats whereas temo landing referrers are great. this is good i like this. i think we can make the following observations\n","\n","\n","- prioritizing temo landing referrer is important, google ads/seo/paidsearch helps bring people in but has less quality\n","\n","\n","- this goes with another chart (reading between the lines) as cluster 2 has more 2,3 lead types (other/service) whereas temo has 1,2 lead types (sales/service) which brings in more money. focusing on temo over google ads is more important and greater money maker\n","\n","\n","- prioritize short and sweet chats as prolonged chat life kills sentiment value, make sure the user is engaged and responded to as quick as possible\n","\n","\n","- prioritize phone numbers and zip codes, cluster 1 has greatest sentiment and generally has zip and phone numbers --- knowing the area of individual as opposed to just their name gives TEMO a better understanding of their userbase. having a phone helps with this geolocational knowledge and provides a personal avenue to speak human-to-human with customers. this makes sense since the number of scams and phishing that occurs with names and emails make these things seem far more distant and less personal. calling a customer and understanding the region they live in (helped by phone whisper and branding) leads to greater sales."]},{"cell_type":"code","execution_count":null,"id":"48a5401e-6bd8-4122-a492-c168db45f1cc","metadata":{"id":"48a5401e-6bd8-4122-a492-c168db45f1cc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f9d82a08-9282-4374-b25e-a3aa517abe19","metadata":{"id":"f9d82a08-9282-4374-b25e-a3aa517abe19"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d0a583a9-18f1-4f03-911f-fd878928b074","metadata":{"id":"d0a583a9-18f1-4f03-911f-fd878928b074"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0dd16683-0b1c-4274-895d-3599944fbf98","metadata":{"id":"0dd16683-0b1c-4274-895d-3599944fbf98"},"outputs":[],"source":["df_path = r'/home/apm204/cs210/Final Project/df_chat.csv'\n","df_chat.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_customer_info_one_hot.csv'\n","df_customer_info_one_hot.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_lead.csv'\n","df_lead.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_location.csv'\n","df_location.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_sentiment.csv'\n","df_sentiment.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_time.csv'\n","df_time.to_csv(df_path, index=False)\n","df_path = r'/home/apm204/cs210/Final Project/df_clusters.csv'\n","df_clusters.to_csv(df_path, index=False)"]},{"cell_type":"code","execution_count":null,"id":"021f4694-fa1e-42c4-9b86-20b8dd618729","metadata":{"id":"021f4694-fa1e-42c4-9b86-20b8dd618729","outputId":"dbfd6809-73fa-4eea-f72c-f42edb63ef7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["1624\n","1620\n"]}],"source":["print(len(df_raw))\n","print(len(combined_df))"]},{"cell_type":"code","execution_count":null,"id":"df888d4b-e85b-4739-b3fb-79ba10d18a99","metadata":{"id":"df888d4b-e85b-4739-b3fb-79ba10d18a99","outputId":"4a58f803-b9a3-4fef-cc28-e180a2e49a8e"},"outputs":[{"data":{"text/plain":["46"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["len(combined_df.columns)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}